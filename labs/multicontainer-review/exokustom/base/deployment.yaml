---
# Source: exochart/charts/cockroachdb/templates/poddisruptionbudget.yaml
kind: PodDisruptionBudget
apiVersion: policy/v1beta1
metadata:
  name: exoplanets-cockroachdb-budget
  namespace: "j-m-r"
  labels:
    helm.sh/chart: cockroachdb-6.0.4
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "exoplanets"
    app.kubernetes.io/managed-by: "Helm"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: cockroachdb
      app.kubernetes.io/instance: "exoplanets"
      app.kubernetes.io/component: cockroachdb
  maxUnavailable: 1
---
# Source: exochart/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: exoplanets-exochart
  labels:
    helm.sh/chart: exochart-0.1.0
    app.kubernetes.io/name: exochart
    app.kubernetes.io/instance: exoplanets
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: exochart/charts/cockroachdb/templates/service.discovery.yaml
# This service only exists to create DNS entries for each pod in
# the StatefulSet such that they can resolve each other's IP addresses.
# It does not create a load-balanced ClusterIP and should not be used directly
# by clients in most circumstances.
kind: Service
apiVersion: v1
metadata:
  name: exoplanets-cockroachdb
  namespace: "j-m-r"
  labels:
    helm.sh/chart: cockroachdb-6.0.4
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "exoplanets"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: cockroachdb
  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon, but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running
    # in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: _status/vars
    prometheus.io/port: "8080"
spec:
  clusterIP: None
  # We want all Pods in the StatefulSet to have their addresses published for
  # the sake of the other CockroachDB Pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    # The main port, served by gRPC, serves Postgres-flavor SQL, inter-node
    # traffic and the CLI.
    - name: "grpc"
      port: 26257
      targetPort: grpc
    # The secondary port serves the UI as well as health and debug endpoints.
    - name: "http"
      port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "exoplanets"
    app.kubernetes.io/component: cockroachdb
---
# Source: exochart/charts/cockroachdb/templates/service.public.yaml
# This Service is meant to be used by clients of the database.
# It exposes a ClusterIP that will automatically load balance connections
# to the different database Pods.
kind: Service
apiVersion: v1
metadata:
  name: exoplanets-cockroachdb-public
  namespace: "j-m-r"
  labels:
    helm.sh/chart: cockroachdb-6.0.4
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "exoplanets"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: cockroachdb
spec:
  type: "ClusterIP"
  ports:
    # The main port, served by gRPC, serves Postgres-flavor SQL, inter-node
    # traffic and the CLI.
    - name: "grpc"
      port: 26257
      targetPort: grpc
    # The secondary port serves the UI as well as health and debug endpoints.
    - name: "http"
      port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "exoplanets"
    app.kubernetes.io/component: cockroachdb
---
# Source: exochart/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: exoplanets-exochart
  labels:
    helm.sh/chart: exochart-0.1.0
    app.kubernetes.io/name: exochart
    app.kubernetes.io/instance: exoplanets
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: exochart
    app.kubernetes.io/instance: exoplanets
---
# Source: exochart/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: exoplanets-exochart
  labels:
    helm.sh/chart: exochart-0.1.0
    app.kubernetes.io/name: exochart
    app.kubernetes.io/instance: exoplanets
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: exochart
      app.kubernetes.io/instance: exoplanets
  template:
    metadata:
      labels:
        app.kubernetes.io/name: exochart
        app.kubernetes.io/instance: exoplanets
    spec:
      serviceAccountName: exoplanets-exochart
      securityContext:
        {}
      containers:
        - name: exochart
          securityContext:
            {}
          image: "quay.io/redhattraining/exoplanets:v1.0"
          imagePullPolicy: IfNotPresent
          env:
          - name: "DB_HOST"
            value: "exoplanets-cockroachdb"
          - name: "DB_NAME"
            value: "postgres"
          - name: "DB_USER"
            value: "root"
          - name: "DB_PORT"
            value: "26257"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}
---
# Source: exochart/charts/cockroachdb/templates/statefulset.yaml
kind: StatefulSet
apiVersion: apps/v1
metadata:
  name: exoplanets-cockroachdb
  namespace: "j-m-r"
  labels:
    helm.sh/chart: cockroachdb-6.0.4
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "exoplanets"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: cockroachdb
spec:
  serviceName: exoplanets-cockroachdb
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: "Parallel"
  selector:
    matchLabels:
      app.kubernetes.io/name: cockroachdb
      app.kubernetes.io/instance: "exoplanets"
      app.kubernetes.io/component: cockroachdb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cockroachdb
        app.kubernetes.io/instance: "exoplanets"
        app.kubernetes.io/component: cockroachdb
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: cockroachdb
                    app.kubernetes.io/instance: "exoplanets"
                    app.kubernetes.io/component: cockroachdb
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: cockroachdb
            app.kubernetes.io/instance: "exoplanets"
            app.kubernetes.io/component: cockroachdb
        maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
      # No pre-stop hook is required, a SIGTERM plus some time is all that's
      # needed for graceful shutdown of a node.
      terminationGracePeriodSeconds: 60
      containers:
        - name: db
          image: "cockroachdb/cockroach:v21.1.3"
          imagePullPolicy: "IfNotPresent"
          args:
            - shell
            - -ecx
            # The use of qualified `hostname -f` is crucial:
            # Other nodes aren't able to look up the unqualified hostname.
            #
            # `--join` CLI flag is hardcoded to exactly 3 Pods, because:
            # 1. Having `--join` value depending on `statefulset.replicas`
            #    will trigger undesired restart of existing Pods when
            #    StatefulSet is scaled up/down. We want to scale without
            #    restarting existing Pods.
            # 2. At least one Pod in `--join` is enough to successfully
            #    join CockroachDB cluster and gossip with all other existing
            #    Pods, even if there are 3 or more Pods.
            # 3. It's harmless for `--join` to have 3 Pods even for 1-Pod
            #    clusters, while it gives us opportunity to scale up even if
            #    some Pods of existing cluster are down (for whatever reason).
            # See details explained here:
            # https://github.com/helm/charts/pull/18993#issuecomment-558795102
            - >-
              exec /cockroach/cockroach
              start --join=${STATEFULSET_NAME}-0.${STATEFULSET_FQDN}:26257,${STATEFULSET_NAME}-1.${STATEFULSET_FQDN}:26257,${STATEFULSET_NAME}-2.${STATEFULSET_FQDN}:26257
              --advertise-host=$(hostname).${STATEFULSET_FQDN}
              --logtostderr=INFO
              --insecure
              --http-port=8080
              --port=26257
              --cache=25%
              --max-sql-memory=25%
          env:
            - name: STATEFULSET_NAME
              value: exoplanets-cockroachdb
            - name: STATEFULSET_FQDN
              value: exoplanets-cockroachdb.j-m-r.svc.cluster.local
            - name: COCKROACH_CHANNEL
              value: kubernetes-helm
          ports:
            - name: grpc
              containerPort: 26257
              protocol: TCP
            - name: http
              containerPort: 8080
              protocol: TCP
          volumeMounts:
            - name: datadir
              mountPath: /cockroach/cockroach-data/
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 5
          readinessProbe:
            httpGet:
              path: /health?ready=1
              port: http
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 2
      volumes:
        - name: datadir
          persistentVolumeClaim:
            claimName: datadir
  volumeClaimTemplates:
    - metadata:
        name: datadir
        labels:
          app.kubernetes.io/name: cockroachdb
          app.kubernetes.io/instance: "exoplanets"
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: "100Gi"
---
# Source: exochart/charts/cockroachdb/templates/tests/client.yaml
kind: Pod
apiVersion: v1
metadata:
  name: exoplanets-cockroachdb-test
  namespace: "j-m-r"
  annotations:
    helm.sh/hook: test-success
spec:
  restartPolicy: Never
  containers:
    - name: client-test
      image: "cockroachdb/cockroach:v21.1.3"
      imagePullPolicy: "IfNotPresent"
      command:
        - /cockroach/cockroach
        - sql
        - --insecure
        - --host
        - exoplanets-cockroachdb-public.j-m-r
        - --port
        - "26257"
        - -e
        - SHOW DATABASES;
---
# Source: exochart/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "exoplanets-exochart-test-connection"
  labels:
    helm.sh/chart: exochart-0.1.0
    app.kubernetes.io/name: exochart
    app.kubernetes.io/instance: exoplanets
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['exoplanets-exochart:80']
  restartPolicy: Never
---
# Source: exochart/charts/cockroachdb/templates/job.init.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: exoplanets-cockroachdb-init
  namespace: "j-m-r"
  labels:
    helm.sh/chart: cockroachdb-6.0.4
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "exoplanets"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: init
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cockroachdb
        app.kubernetes.io/instance: "exoplanets"
        app.kubernetes.io/component: init
    spec:
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      containers:
        - name: cluster-init
          image: "cockroachdb/cockroach:v21.1.3"
          imagePullPolicy: "IfNotPresent"
          # Run the command in an `while true` loop because this Job is bound
          # to come up before the CockroachDB Pods (due to the time needed to
          # get PersistentVolumes attached to Nodes), and sleeping 5 seconds
          # between attempts is much better than letting the Pod fail when
          # the init command does and waiting out Kubernetes' non-configurable
          # exponential back-off for Pod restarts.
          # Command completes either when cluster initialization succeeds,
          # or when cluster has been initialized already.
          command:
            - /bin/bash
            - -c
            - >-
              while true; do
              initOUT=$(set -x;
              /cockroach/cockroach init
              --insecure
              --host=exoplanets-cockroachdb-0.exoplanets-cockroachdb:26257
              2>&1);
              initRC="$?";
              echo $initOUT;
              [[ "$initRC" == "0" ]] && exit 0;
              [[ "$initOUT" == *"cluster has already been initialized"* ]] && exit 0;
              sleep 5;
              done
